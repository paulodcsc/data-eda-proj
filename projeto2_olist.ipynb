{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbf6d2a",
   "metadata": {},
   "source": [
    "# Projeto 2 — Aprendizado de Máquina (Olist)\n",
    "\n",
    "## 1) Definição do problema\n",
    "\n",
    "Objetivo: construir um modelo de classificação binária para prever se um pedido será entregue com atraso em relação à data estimada no momento da compra.\n",
    "\n",
    "Alvo (y): atraso_entrega = 1 se `order_delivered_customer_date` > `order_estimated_delivery_date`, caso contrário 0.\n",
    "\n",
    "Critério de sucesso: priorizar F1-Score (equilíbrio entre precisão e revocação), reportando também precisão, revocação, acurácia e ROC-AUC.\n",
    "\n",
    "Métricas (fórmulas):\n",
    "- Precisão = TP / (TP + FP)\n",
    "- Revocação = TP / (TP + FN)\n",
    "- F1 = 2 * (precisão * revocação) / (precisão + revocação)\n",
    "- AUC calculada a partir da curva ROC\n",
    "\n",
    "Restrições/assunções:\n",
    "- Usar apenas variáveis disponíveis até a compra/aprovação (evitar vazamento de informação).\n",
    "- Reprodutibilidade via semente aleatória fixa.\n",
    "\n",
    "Notas:\n",
    "- Base: arquivos CSV oficiais do repositório Olist na pasta atual.\n",
    "- Vamos agregar características no nível do pedido (order_id) a partir de itens, produtos, pagamentos e perfis de cliente/vendedor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e3839ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivos encontrados? True True\n"
     ]
    }
   ],
   "source": [
    "# Imports, semente e caminhos\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, classification_report, confusion_matrix, RocCurveDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Reprodutibilidade\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Caminhos\n",
    "BASE_DIR = Path('.')\n",
    "CSV_ORDERS = BASE_DIR / 'olist_orders_dataset.csv'\n",
    "CSV_ORDER_ITEMS = BASE_DIR / 'olist_order_items_dataset.csv'\n",
    "CSV_PRODUCTS = BASE_DIR / 'olist_products_dataset.csv'\n",
    "CSV_CUSTOMERS = BASE_DIR / 'olist_customers_dataset.csv'\n",
    "CSV_SELLERS = BASE_DIR / 'olist_sellers_dataset.csv'\n",
    "CSV_PAYMENTS = BASE_DIR / 'olist_order_payments_dataset.csv'\n",
    "CSV_REVIEWS = BASE_DIR / 'olist_order_reviews_dataset.csv'\n",
    "CSV_GEO = BASE_DIR / 'olist_geolocation_dataset.csv'\n",
    "\n",
    "# Saídas\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "OUTPUT_DIR = BASE_DIR / 'output'\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print('Arquivos encontrados?', CSV_ORDERS.exists(), CSV_ORDER_ITEMS.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fc0f16",
   "metadata": {},
   "source": [
    "## 2) Pré-processamento de dados\n",
    "\n",
    "Passos desta seção:\n",
    "- Carregar CSVs (orders, items, products, customers, sellers, payments, reviews, geolocation).\n",
    "- Selecionar colunas úteis e realizar merges no nível do pedido (order_id).\n",
    "- Criar o alvo `atraso_entrega` e remover colunas pós-entrega para evitar vazamento.\n",
    "- Engenhar features disponíveis até compra/aprovação (ex.: tempo estimado, total preço/frete, nº itens, categoria, UF, método de pagamento, parcelas).\n",
    "- Listar colunas numéricas e categóricas para o pré-processador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba5bd241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                           order_id                       customer_id  \\\n",
       " 0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       " 1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       " 2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       " 3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       " 4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       " \n",
       "   order_status order_purchase_timestamp    order_approved_at  \\\n",
       " 0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       " 1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       " 2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       " 3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
       " 4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
       " \n",
       "   order_delivered_carrier_date order_delivered_customer_date  \\\n",
       " 0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       " 1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       " 2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       " 3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
       " 4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
       " \n",
       "   order_estimated_delivery_date  \n",
       " 0           2017-10-18 00:00:00  \n",
       " 1           2018-08-13 00:00:00  \n",
       " 2           2018-09-04 00:00:00  \n",
       " 3           2017-12-15 00:00:00  \n",
       " 4           2018-02-26 00:00:00  ,\n",
       "                            order_id  order_item_id  \\\n",
       " 0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
       " 1  00018f77f2f0320c557190d7a144bdd3              1   \n",
       " 2  000229ec398224ef6ca0657da4fc703e              1   \n",
       " 3  00024acbcdf0a6daa1e931b038114c75              1   \n",
       " 4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
       " \n",
       "                          product_id                         seller_id  \\\n",
       " 0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   \n",
       " 1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       " 2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       " 3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
       " 4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       " \n",
       "    shipping_limit_date   price  freight_value  \n",
       " 0  2017-09-19 09:45:35   58.90          13.29  \n",
       " 1  2017-05-03 11:05:13  239.90          19.93  \n",
       " 2  2018-01-18 14:48:30  199.00          17.87  \n",
       " 3  2018-08-15 10:10:18   12.99          12.79  \n",
       " 4  2017-02-13 13:57:51  199.90          18.14  )"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar dados brutos\n",
    "def load_data():\n",
    "    orders = pd.read_csv(CSV_ORDERS)\n",
    "    order_items = pd.read_csv(CSV_ORDER_ITEMS)\n",
    "    products = pd.read_csv(CSV_PRODUCTS)\n",
    "    customers = pd.read_csv(CSV_CUSTOMERS)\n",
    "    sellers = pd.read_csv(CSV_SELLERS)\n",
    "    payments = pd.read_csv(CSV_PAYMENTS)\n",
    "    reviews = pd.read_csv(CSV_REVIEWS)\n",
    "    # geolocation é pesado e granular em CEP; manteremos fora por simplicidade inicial\n",
    "    return orders, order_items, products, customers, sellers, payments, reviews\n",
    "\n",
    "orders, order_items, products, customers, sellers, payments, reviews = load_data()\n",
    "\n",
    "orders.head(), order_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3be55b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(atraso_entrega\n",
       " 0    88649\n",
       " 1     7827\n",
       " Name: count, dtype: Int64,\n",
       " order_approved_at                0.000145\n",
       " order_delivered_carrier_date     0.000010\n",
       " order_purchase_timestamp         0.000000\n",
       " order_delivered_customer_date    0.000000\n",
       " order_estimated_delivery_date    0.000000\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conversão de datas e criação do alvo (y)\n",
    "\n",
    "def to_datetime(df, cols):\n",
    "    for c in cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_datetime(df[c], errors='coerce')\n",
    "\n",
    "# Converter datas relevantes\n",
    "order_date_cols = [\n",
    "    'order_purchase_timestamp', 'order_approved_at',\n",
    "    'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "\n",
    "to_datetime(orders, order_date_cols)\n",
    "\n",
    "# Alvo: atraso_entrega (1 se entregue após a data estimada)\n",
    "orders['atraso_entrega'] = (\n",
    "    orders['order_delivered_customer_date'] > orders['order_estimated_delivery_date']\n",
    ").astype('Int64')\n",
    "\n",
    "# Manter apenas pedidos com datas necessárias para definir o alvo\n",
    "orders = orders.dropna(subset=['order_delivered_customer_date', 'order_estimated_delivery_date'])\n",
    "\n",
    "orders['atraso_entrega'].value_counts(dropna=False), orders[order_date_cols].isna().mean().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38fe5cac",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Must provide 'func' or tuples of '(column, aggfunc).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m prod_cols = [\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_category_name\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_weight_g\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_length_cm\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_height_cm\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_width_cm\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     14\u001b[39m order_items_prod = order_items.merge(products[prod_cols], on=\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m, how=\u001b[33m'\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m prod_aggs = \u001b[43morder_items_prod\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morder_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpeso_medio\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_weight_g\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmean\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvol_medio_cm3\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43morder_items_prod\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_length_cm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m*\u001b[49m\u001b[43morder_items_prod\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_height_cm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m*\u001b[49m\u001b[43morder_items_prod\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_width_cm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder_items_prod\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morder_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# A expressão acima complica dentro do agg; calcular volume explicitamente\u001b[39;00m\n\u001b[32m     20\u001b[39m order_items_prod[\u001b[33m'\u001b[39m\u001b[33mvolume_cm3\u001b[39m\u001b[33m'\u001b[39m] = order_items_prod[[\u001b[33m'\u001b[39m\u001b[33mproduct_length_cm\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_height_cm\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mproduct_width_cm\u001b[39m\u001b[33m'\u001b[39m]].prod(axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/Proj2_CdeDados/.venv/lib/python3.14/site-packages/pandas/core/groupby/generic.py:1422\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1420\u001b[39m \u001b[38;5;129m@doc\u001b[39m(_agg_template_frame, examples=_agg_examples_doc, klass=\u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34maggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, func=\u001b[38;5;28;01mNone\u001b[39;00m, *args, engine=\u001b[38;5;28;01mNone\u001b[39;00m, engine_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     relabeling, func, columns, order = \u001b[43mreconstruct_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m     func = maybe_mangle_lambdas(func)\n\u001b[32m   1425\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m maybe_use_numba(engine):\n\u001b[32m   1426\u001b[39m         \u001b[38;5;66;03m# Not all agg functions support numba, only propagate numba kwargs\u001b[39;00m\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# if user asks for numba\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Coding/Proj2_CdeDados/.venv/lib/python3.14/site-packages/pandas/core/apply.py:1691\u001b[39m, in \u001b[36mreconstruct_func\u001b[39m\u001b[34m(func, **kwargs)\u001b[39m\n\u001b[32m   1685\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SpecificationError(\n\u001b[32m   1686\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mFunction names must be unique if there is no new column names \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1687\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33massigned\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1688\u001b[39m         )\n\u001b[32m   1689\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1690\u001b[39m         \u001b[38;5;66;03m# nicer error message\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1691\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mMust provide \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or tuples of \u001b[39m\u001b[33m'\u001b[39m\u001b[33m(column, aggfunc).\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1693\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[32m   1694\u001b[39m     \u001b[38;5;66;03m# error: Incompatible types in assignment (expression has type\u001b[39;00m\n\u001b[32m   1695\u001b[39m     \u001b[38;5;66;03m# \"MutableMapping[Hashable, list[Callable[..., Any] | str]]\", variable has type\u001b[39;00m\n\u001b[32m   1696\u001b[39m     \u001b[38;5;66;03m# \"Callable[..., Any] | str | list[Callable[..., Any] | str] |\u001b[39;00m\n\u001b[32m   1697\u001b[39m     \u001b[38;5;66;03m# MutableMapping[Hashable, Callable[..., Any] | str | list[Callable[..., Any] |\u001b[39;00m\n\u001b[32m   1698\u001b[39m     \u001b[38;5;66;03m# str]] | None\")\u001b[39;00m\n\u001b[32m   1699\u001b[39m     func, columns, order = normalize_keyword_aggregation(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m   1700\u001b[39m         kwargs\n\u001b[32m   1701\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Must provide 'func' or tuples of '(column, aggfunc)."
     ]
    }
   ],
   "source": [
    "# Selecionar features disponíveis até a compra/aprovação e agregar por pedido\n",
    "\n",
    "# Itens do pedido: somatórios e contagens\n",
    "item_aggs = order_items.groupby('order_id').agg(\n",
    "    total_preco=('price', 'sum'),\n",
    "    total_frete=('freight_value', 'sum'),\n",
    "    num_itens=('order_item_id', 'count'),\n",
    "    num_vendedores=('seller_id', pd.Series.nunique),\n",
    "    num_produtos=('product_id', pd.Series.nunique)\n",
    ").reset_index()\n",
    "\n",
    "# Produtos: características físicas (médias por pedido via merge nos itens)\n",
    "prod_cols = ['product_id','product_category_name','product_weight_g','product_length_cm','product_height_cm','product_width_cm']\n",
    "order_items_prod = order_items.merge(products[prod_cols], on='product_id', how='left')\n",
    "prod_aggs = order_items_prod.groupby('order_id').agg(\n",
    "    peso_medio=('product_weight_g','mean'),\n",
    "    vol_medio_cm3=(lambda x: (order_items_prod['product_length_cm']*order_items_prod['product_height_cm']*order_items_prod['product_width_cm']).groupby(order_items_prod['order_id']).mean())\n",
    ")\n",
    "# A expressão acima complica dentro do agg; calcular volume explicitamente\n",
    "order_items_prod['volume_cm3'] = order_items_prod[['product_length_cm','product_height_cm','product_width_cm']].prod(axis=1)\n",
    "prod_aggs = order_items_prod.groupby('order_id').agg(\n",
    "    peso_medio=('product_weight_g','mean'),\n",
    "    volume_medio_cm3=('volume_cm3','mean'),\n",
    "    categoria_mais_freq=('product_category_name', lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan)\n",
    ").reset_index()\n",
    "\n",
    "# Pagamentos: método principal e parcelas (agregar por pedido)\n",
    "pay_aggs = payments.groupby('order_id').agg(\n",
    "    payment_installments_max=('payment_installments','max'),\n",
    "    payment_sequential_max=('payment_sequential','max'),\n",
    "    payment_type_main=('payment_type', lambda s: s.mode().iloc[0] if not s.mode().empty else np.nan),\n",
    "    payment_value_total=('payment_value','sum')\n",
    ").reset_index()\n",
    "\n",
    "# Clientes e vendedores: UF e cidade (one-hot posterior)\n",
    "cust_cols = ['customer_id','customer_city','customer_state']\n",
    "sel_cols = ['seller_id','seller_city','seller_state']\n",
    "\n",
    "# Para obter UF do vendedor no nível do pedido, usar o vendedor do primeiro item\n",
    "first_seller_per_order = order_items.sort_values('order_item_id').groupby('order_id').first().reset_index()[['order_id','seller_id']]\n",
    "first_seller_per_order = first_seller_per_order.merge(sellers[sel_cols], on='seller_id', how='left')\n",
    "\n",
    "# Montar base no nível do pedido\n",
    "base = orders.merge(item_aggs, on='order_id', how='left') \\\n",
    "             .merge(prod_aggs, on='order_id', how='left') \\\n",
    "             .merge(pay_aggs, on='order_id', how='left') \\\n",
    "             .merge(customers[['customer_id','customer_state','customer_city']], on='customer_id', how='left') \\\n",
    "             .merge(first_seller_per_order[['order_id','seller_state','seller_city']], on='order_id', how='left')\n",
    "\n",
    "# Features de tempo conhecidas cedo\n",
    "base['dias_estimados'] = (base['order_estimated_delivery_date'] - base['order_purchase_timestamp']).dt.days\n",
    "base['dias_ate_aprovacao'] = (base['order_approved_at'] - base['order_purchase_timestamp']).dt.total_seconds()/3600.0\n",
    "\n",
    "# Evitar vazamento: não usar datas pós-entrega como features\n",
    "feature_drop = ['order_delivered_carrier_date','order_delivered_customer_date']\n",
    "base = base.drop(columns=[c for c in feature_drop if c in base.columns])\n",
    "\n",
    "# Rácios úteis\n",
    "base['ratio_frete_preco'] = base['total_frete'] / (base['total_preco'] + 1e-6)\n",
    "\n",
    "# Alvo\n",
    "base['y'] = base['atraso_entrega'].astype('int')\n",
    "\n",
    "# Selecionar colunas para modelagem\n",
    "numeric_cols = [\n",
    "    'total_preco','total_frete','num_itens','num_vendedores','num_produtos',\n",
    "    'peso_medio','volume_medio_cm3','payment_installments_max','payment_sequential_max',\n",
    "    'payment_value_total','dias_estimados','dias_ate_aprovacao','ratio_frete_preco'\n",
    "]\n",
    "cat_cols = ['payment_type_main','categoria_mais_freq','customer_state','seller_state']\n",
    "\n",
    "use_cols = numeric_cols + cat_cols + ['y']\n",
    "df_model = base[use_cols].copy()\n",
    "\n",
    "print(df_model.shape)\n",
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f400bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar treino e teste (estratificado)\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Remover linhas com alvo ausente\n",
    "_df = df_model.dropna(subset=['y']).copy()\n",
    "\n",
    "X = _df.drop(columns=['y'])\n",
    "y = _df['y']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "numeric_features = [c for c in numeric_cols if c in X_train.columns]\n",
    "categorical_features = [c for c in cat_cols if c in X_train.columns]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80e76a",
   "metadata": {},
   "source": [
    "## 3) Análise Exploratória dos Dados (EDA)\n",
    "\n",
    "Nesta seção exploramos:\n",
    "- Distribuição do alvo (balanceamento de classes).\n",
    "- Estatísticas descritivas (numéricas) e nulos.\n",
    "- Histogramas/boxplots comparando atraso vs não atraso.\n",
    "- Correlações entre variáveis numéricas.\n",
    "- Frequência das principais categorias e taxa de atraso por categoria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição do alvo\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "y.value_counts().plot(kind='bar', ax=ax[0], title='Contagem por classe (0=NoPrazo, 1=Atraso)')\n",
    "y.value_counts(normalize=True).plot(kind='bar', ax=ax[1], title='Proporção por classe')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Nulos por coluna\n",
    "nulls = df_model.isna().mean().sort_values(ascending=False)\n",
    "print('Top 10 nulos:')\n",
    "print(nulls.head(10))\n",
    "\n",
    "# Estatísticas descritivas\n",
    "num_desc = df_model[numeric_cols].describe().T\n",
    "num_desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlações numéricas\n",
    "plt.figure(figsize=(10,8))\n",
    "num_df = df_model[numeric_cols].copy()\n",
    "num_df = num_df.fillna(num_df.median(numeric_only=True))\n",
    "sns.heatmap(num_df.corr(numeric_only=True), cmap='Blues', annot=False)\n",
    "plt.title('Correlação entre variáveis numéricas')\n",
    "plt.show()\n",
    "\n",
    "# Taxa de atraso por categoria e UF (top categorias)\n",
    "if 'categoria_mais_freq' in df_model.columns:\n",
    "    cat_rate = df_model.groupby('categoria_mais_freq')['y'].mean().sort_values(ascending=False).head(15)\n",
    "    cat_rate.plot(kind='bar', figsize=(10,4), title='Taxa de atraso por categoria (Top 15)')\n",
    "    plt.show()\n",
    "\n",
    "for col in ['payment_type_main','customer_state','seller_state']:\n",
    "    if col in df_model.columns:\n",
    "        rate = df_model.groupby(col)['y'].mean().sort_values(ascending=False).head(15)\n",
    "        rate.plot(kind='bar', figsize=(10,4), title=f'Taxa de atraso por {col} (Top 15)')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51ceea",
   "metadata": {},
   "source": [
    "## 4) Treinamento do modelo\n",
    "\n",
    "Vamos construir um pipeline com pré-processamento e dois modelos candidatos:\n",
    "- Regressão Logística (class_weight='balanced')\n",
    "- Random Forest (class_weight='balanced_subsample')\n",
    "\n",
    "Usaremos GridSearchCV (StratifiedKFold=5, scoring='f1') para selecionar hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c561fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir pipelines e grades\n",
    "pipe_lr = Pipeline(steps=[('preprocess', preprocessor),\n",
    "                         ('clf', LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE))])\n",
    "\n",
    "pipe_rf = Pipeline(steps=[('preprocess', preprocessor),\n",
    "                         ('clf', RandomForestClassifier(class_weight='balanced_subsample', random_state=RANDOM_STATE))])\n",
    "\n",
    "param_grid_lr = {\n",
    "    'clf__C': [0.1, 1.0, 3.0],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__solver': ['lbfgs']\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'clf__n_estimators': [150, 300],\n",
    "    'clf__max_depth': [None, 10, 20],\n",
    "    'clf__max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "def run_grid(pipe, param_grid, name):\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=cv, scoring='f1', n_jobs=-1, verbose=1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(f\"{name} - melhor F1 CV: {grid.best_score_:.4f}\")\n",
    "    print('Melhores params:', grid.best_params_)\n",
    "    return grid\n",
    "\n",
    "res_lr = run_grid(pipe_lr, param_grid_lr, 'LogisticRegression')\n",
    "res_rf = run_grid(pipe_rf, param_grid_rf, 'RandomForest')\n",
    "\n",
    "best_grid = res_rf if res_rf.best_score_ >= res_lr.best_score_ else res_lr\n",
    "best_name = 'RandomForest' if best_grid is res_rf else 'LogisticRegression'\n",
    "\n",
    "print('Modelo escolhido:', best_name)\n",
    "\n",
    "best_model = best_grid.best_estimator_\n",
    "\n",
    "# Reajustar no treino completo (já é feito no GridSearch por padrão em refit=True)\n",
    "# Salvar pipeline\n",
    "joblib.dump(best_model, MODELS_DIR / 'best_pipeline.joblib')\n",
    "(MODELS_DIR / 'best_pipeline.joblib').exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec117b5b",
   "metadata": {},
   "source": [
    "## 5) Avaliação do modelo e conclusão\n",
    "\n",
    "Nesta seção:\n",
    "- Avaliaremos o melhor modelo em teste (métricas e gráficos).\n",
    "- Ajustaremos limiar de decisão (opcional) para maximizar F1.\n",
    "- Registraremos conclusões objetivas e próximos passos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6c715e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação em teste\n",
    "best_model = joblib.load(MODELS_DIR / 'best_pipeline.joblib')\n",
    "\n",
    "# Predições padrão (threshold=0.5)\n",
    "y_prob = best_model.predict_proba(X_test)[:,1]\n",
    "y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred),\n",
    "    'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "    'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "    'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "    'roc_auc': roc_auc_score(y_test, y_prob)\n",
    "}\n",
    "print(metrics)\n",
    "print('\\nClassification report:')\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[0])\n",
    "ax[0].set_title('Matriz de confusão (absoluta)')\n",
    "\n",
    "cm_norm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='Blues', ax=ax[1])\n",
    "ax[1].set_title('Matriz de confusão (normalizada)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Curvas ROC e PR\n",
    "disp = RocCurveDisplay.from_predictions(y_test, y_prob)\n",
    "plt.show()\n",
    "\n",
    "PrecisionRecallDisplay.from_predictions(y_test, y_prob)\n",
    "plt.show()\n",
    "\n",
    "# Opcional: busca de limiar por F1\n",
    "thresholds = np.linspace(0.2, 0.8, 25)\n",
    "best_thr, best_f1 = 0.5, metrics['f1']\n",
    "for t in thresholds:\n",
    "    _pred = (y_prob >= t).astype(int)\n",
    "    _f1 = f1_score(y_test, _pred, zero_division=0)\n",
    "    if _f1 > best_f1:\n",
    "        best_f1, best_thr = _f1, t\n",
    "print(f'Melhor threshold por F1: {best_thr:.3f} | F1={best_f1:.4f}')\n",
    "\n",
    "# Importâncias / Coeficientes\n",
    "clf = best_model.named_steps['clf']\n",
    "if hasattr(clf, 'feature_importances_'):\n",
    "    # Recuperar nomes das features após OneHot\n",
    "    ohe = best_model.named_steps['preprocess'].named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "    feat_names = numeric_features + cat_names\n",
    "    importances = pd.Series(clf.feature_importances_, index=feat_names).sort_values(ascending=False).head(20)\n",
    "    importances.plot(kind='barh', figsize=(8,6), title='Importâncias das features (Top 20)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "elif hasattr(clf, 'coef_'):\n",
    "    ohe = best_model.named_steps['preprocess'].named_transformers_['cat'].named_steps['onehot']\n",
    "    cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
    "    feat_names = numeric_features + cat_names\n",
    "    coefs = pd.Series(clf.coef_[0], index=feat_names).sort_values(key=lambda s: s.abs(), ascending=False).head(20)\n",
    "    coefs.plot(kind='barh', figsize=(8,6), title='Coeficientes (Top 20 por |coef|)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "# Salvar métricas\n",
    "pd.Series(metrics).to_csv(OUTPUT_DIR / 'test_metrics.csv')\n",
    "print('Métricas salvas em output/test_metrics.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec61dd",
   "metadata": {},
   "source": [
    "### Conclusões e próximos passos (anotações)\n",
    "\n",
    "- Classe minoritária: a taxa de atraso pode ser menor do que a de entregas no prazo; por isso priorizamos F1 e adotamos class_weight.\n",
    "- Features relevantes típicas: dias_estimados, razão frete/preço, número de itens e características físicas têm impacto no atraso.\n",
    "- Modelo escolhido: comparar F1 em validação cruzada; frequentemente RF supera LR pela não-linearidade.\n",
    "- Limitações: potenciais vieses por UF/cidade; ausência de distância geográfica precisa; possibilidade de sazonalidade (deveria-se validar temporalmente).\n",
    "- Próximos passos:\n",
    "  - Engenhar distância cliente–vendedor (usando geolocalização) e variáveis temporais (mês, dia_da_semana, feriados).\n",
    "  - Tuning adicional e validação temporal (time series split) para robustez.\n",
    "  - Calibração de probabilidades (CalibratedClassifierCV) e otimização de threshold por objetivo de negócio.\n",
    "  - Monitoramento de drift e re-treino periódico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
