{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f5155be",
   "metadata": {},
   "source": [
    "# Pipeline de Pré-processamento\n",
    "\n",
    "Este notebook descreve a etapa 2 do trabalho: limpeza, engenharia e transformação dos dados da Olist para gerar um conjunto adequado ao treinamento, sempre usando o dataset completo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6354ca",
   "metadata": {},
   "source": [
    "## Visão geral\n",
    "\n",
    "1. Carregar as tabelas brutas da Olist e derivar a variável alvo e agregações.\n",
    "2. Normalizar textos (maiúsculas, remoção de acentos, `_`).\n",
    "3. Usar `ColumnTransformer` para imputação, escalonamento e codificação.\n",
    "4. Exportar o resultado compacto em `data/clean/olist_ml_ready.csv`.\n",
    "5. Visualizar indicadores com Plotly para toda a base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9c434",
   "metadata": {},
   "source": [
    "### Bibliotecas e configurações\n",
    "\n",
    "Este bloco importa geopandas, pandas, plotly e scikit-learn, além de definir os diretórios para leitura e escrita dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6b5572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "\n",
    "RAW_DIR = Path('/home/arthur/projects/data-eda-proj/data/raw')\n",
    "CLEAN_DIR = Path('/home/arthur/projects/data-eda-proj/data/clean')\n",
    "CLEAN_DIR.mkdir(exist_ok=True)\n",
    "TARGET_FILE = CLEAN_DIR / 'olist_ml_ready.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3eec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(value):\n",
    "    if pd.isna(value):\n",
    "        return value\n",
    "    text = str(value).strip().upper()\n",
    "    stripped = ''.join(\n",
    "        ch for ch in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(ch) != 'Mn'\n",
    "    )\n",
    "    return stripped.replace(' ', '_')\n",
    "\n",
    "def normalize_frame(dataframe):\n",
    "    return dataframe.applymap(normalize_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed50d1",
   "metadata": {},
   "source": [
    "### Carregando as tabelas brutas\n",
    "\n",
    "Lê todas as CSVs oficiais da Olist que serão consolidadas no dataset final (pedidos, itens, clientes, sellers, geolocalizações, pagamentos e reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe89bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = pd.read_csv(\n",
    "    RAW_DIR / 'olist_orders_dataset.csv',\n",
    "    parse_dates=[\n",
    "        'order_purchase_timestamp',\n",
    "        'order_approved_at',\n",
    "        'order_delivered_carrier_date',\n",
    "        'order_delivered_customer_date',\n",
    "        'order_estimated_delivery_date'\n",
    "    ]\n",
    ")\n",
    "order_items = pd.read_csv(\n",
    "    RAW_DIR / 'olist_order_items_dataset.csv',\n",
    "    parse_dates=['shipping_limit_date']\n",
    ")\n",
    "customers = pd.read_csv(RAW_DIR / 'olist_customers_dataset.csv')\n",
    "sellers = pd.read_csv(RAW_DIR / 'olist_sellers_dataset.csv')\n",
    "geolocation = pd.read_csv(RAW_DIR / 'olist_geolocation_dataset.csv')\n",
    "payments = pd.read_csv(RAW_DIR / 'olist_order_payments_dataset.csv')\n",
    "reviews = pd.read_csv(RAW_DIR / 'olist_order_reviews_dataset.csv')\n",
    "products = pd.read_csv(RAW_DIR / 'olist_products_dataset.csv')\n",
    "category_translation = pd.read_csv(RAW_DIR / 'product_category_name_translation.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc08e90",
   "metadata": {},
   "source": [
    "### Engenharia temporal e agregações\n",
    "\n",
    "Cálculo da variável alvo (`delivery_time_days`), atrasos de aprovação/estimativa e agregações por pedido (quantidade de itens, preços, categoria principal, pagamentos e reviews)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618e65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = orders[orders['order_delivered_customer_date'].notna()].copy()\n",
    "orders['delivery_time_days'] = (\n",
    "    (orders['order_delivered_customer_date'] - orders['order_purchase_timestamp']).dt.total_seconds() / 86400\n",
    ")\n",
    "orders['approval_delay_hours'] = (\n",
    "    (orders['order_approved_at'] - orders['order_purchase_timestamp']).dt.total_seconds() / 3600\n",
    ")\n",
    "orders['delivery_estimate_gap_days'] = (\n",
    "    (orders['order_estimated_delivery_date'] - orders['order_delivered_customer_date']).dt.total_seconds() / 86400\n",
    ")\n",
    "orders = orders[orders['delivery_time_days'] >= 0]\n",
    "\n",
    "category_translation = category_translation.fillna('UNKNOWN')\n",
    "products = products.merge(\n",
    "    category_translation, on='product_category_name', how='left'\n",
    ")\n",
    "products['product_category_name_english'] = products['product_category_name_english'].fillna('UNKNOWN').astype(str)\n",
    "order_items = order_items.merge(\n",
    "    products[['product_id', 'product_category_name_english']],\n",
    "    on='product_id', how='left'\n",
    ")\n",
    "items_agg = (\n",
    "    order_items.groupby('order_id', as_index=False)\n",
    "    .agg(\n",
    "        items_count=('order_item_id', 'count'),\n",
    "        unique_sellers=('seller_id', 'nunique'),\n",
    "        unique_products=('product_id', 'nunique'),\n",
    "        total_price=('price', 'sum'),\n",
    "        total_freight=('freight_value', 'sum')\n",
    "    )\n",
    ")\n",
    "items_agg['price_per_item'] = items_agg['total_price'] / items_agg['items_count']\n",
    "category_mode = (\n",
    "    order_items.groupby('order_id')\n",
    "    .agg(primary_category=('product_category_name_english', lambda s: s.mode().iloc[0] if not s.mode().empty else 'UNKNOWN'))\n",
    "    .reset_index()\n",
    ")\n",
    "payment_agg = (\n",
    "    payments.groupby('order_id', as_index=False)\n",
    "    .agg(\n",
    "        payment_value_sum=('payment_value', 'sum'),\n",
    "        payment_installments_max=('payment_installments', 'max'),\n",
    "        payment_type=('payment_type', 'first')\n",
    "    )\n",
    ")\n",
    "review_agg = (\n",
    "    reviews.groupby('order_id', as_index=False)\n",
    "    .agg(\n",
    "        average_review_score=('review_score', 'mean'),\n",
    "        review_count=('review_id', 'count')\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de8156d",
   "metadata": {},
   "source": [
    "### Combinação com geolocalizações\n",
    "\n",
    "Integra as latitudes/longitudes do cliente e do seller usando a tabela de CEPs para habilitar análises espaciais e cálculo de distâncias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9a884c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = customers.merge(\n",
    "    geolocation.groupby('geolocation_zip_code_prefix', as_index=False)\n",
    "    .agg(latitude=('geolocation_lat', 'median'), longitude=('geolocation_lng', 'median')),\n",
    "    left_on='customer_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left'\n",
    ")\n",
    "customers.rename(columns={'latitude': 'customer_latitude', 'longitude': 'customer_longitude'}, inplace=True)\n",
    "\n",
    "sellers = sellers.merge(\n",
    "    geolocation.groupby('geolocation_zip_code_prefix', as_index=False)\n",
    "    .agg(latitude=('geolocation_lat', 'median'), longitude=('geolocation_lng', 'median')),\n",
    "    left_on='seller_zip_code_prefix', right_on='geolocation_zip_code_prefix', how='left'\n",
    ")\n",
    "sellers.rename(columns={'latitude': 'seller_latitude', 'longitude': 'seller_longitude'}, inplace=True)\n",
    "\n",
    "seller_for_order = (\n",
    "    order_items.sort_values('order_item_id')\n",
    "    .drop_duplicates('order_id')\n",
    "    .merge(\n",
    "        sellers[['seller_id', 'seller_state', 'seller_latitude', 'seller_longitude']],\n",
    "        on='seller_id', how='left'\n",
    "    )\n",
    "    .rename(columns={'seller_state': 'primary_seller_state'})\n",
    "    [['order_id', 'primary_seller_state', 'seller_latitude', 'seller_longitude']]\n",
    ")\n",
    "\n",
    "dataset = (\n",
    "    orders\n",
    "    .merge(items_agg, on='order_id', how='left')\n",
    "    .merge(category_mode, on='order_id', how='left')\n",
    "    .merge(payment_agg, on='order_id', how='left')\n",
    "    .merge(review_agg, on='order_id', how='left')\n",
    "    .merge(\n",
    "        customers[['customer_id', 'customer_state', 'customer_latitude', 'customer_longitude']],\n",
    "        on='customer_id', how='left'\n",
    "    )\n",
    "    .merge(seller_for_order, on='order_id', how='left')\n",
    ")\n",
    "dataset['order_month'] = dataset['order_purchase_timestamp'].dt.month\n",
    "dataset['order_weekday'] = dataset['order_purchase_timestamp'].dt.dayofweek\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56da8914",
   "metadata": {},
   "source": [
    "### Distâncias geográficas\n",
    "\n",
    "Remove registros sem coordenadas completas e usa GeoPandas para calcular a distância real em quilômetros entre o cliente e o seller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9126bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset limpo com 95998 linhas e 31 colunas.\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.dropna(subset=[\n",
    "    'customer_latitude', 'customer_longitude', 'seller_latitude', 'seller_longitude'\n",
    "]).copy()\n",
    "print(f'Dataset limpo com {len(dataset)} linhas e {dataset.shape[1]} colunas.')\n",
    "\n",
    "customer_points = gpd.GeoSeries(\n",
    "    gpd.points_from_xy(dataset['customer_longitude'], dataset['customer_latitude']),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "seller_points = gpd.GeoSeries(\n",
    "    gpd.points_from_xy(dataset['seller_longitude'], dataset['seller_latitude']),\n",
    "    crs='EPSG:4326'\n",
    ")\n",
    "dataset['distance_km'] = (\n",
    "    customer_points.to_crs('EPSG:3857')\n",
    "    .distance(seller_points.to_crs('EPSG:3857'))\n",
    "    / 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77025792",
   "metadata": {},
   "source": [
    "### Normalização textual\n",
    "\n",
    "Aplica funções que convertem textos para maiúsculas, removem acentos e trocam espaços por `_` nos atributos categóricos antes da codificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b86ea49",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.drop(columns=['customer_city', 'primary_seller_city'], inplace=True, errors='ignore')\n",
    "categorical_columns = ['customer_state', 'primary_seller_state', 'payment_type', 'primary_category']\n",
    "numeric_columns = [\n",
    "    'items_count', 'unique_sellers', 'unique_products', 'total_price', 'total_freight',\n",
    "    'price_per_item', 'payment_value_sum', 'payment_installments_max', 'average_review_score',\n",
    "    'review_count', 'customer_latitude', 'customer_longitude', 'seller_latitude', 'seller_longitude',\n",
    "    'order_month', 'order_weekday', 'distance_km'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce30400",
   "metadata": {},
   "source": [
    "### Pipeline de transformação\n",
    "\n",
    "Configura o `ColumnTransformer` com pipelines separados para atributos categóricos (normalização, imputação e OneHot) e numéricos (imputação e `StandardScaler`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419667c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_pipeline = Pipeline([\n",
    "    ('normalizer', FunctionTransformer(normalize_frame, validate=False)),\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
    "])\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('categorical', cat_pipeline, categorical_columns),\n",
    "    ('numerical', num_pipeline, numeric_columns)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9087ba28",
   "metadata": {},
   "source": [
    "### Aplicação final e exportação\n",
    "\n",
    "Transforma as features, adiciona a variável alvo e grava o CSV pronto em `data/clean/olist_ml_ready.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878f50f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61759/3644485920.py:12: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return dataframe.applymap(normalize_text)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Estimator normalizer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m feature_df = dataset[categorical_columns + numeric_columns].copy()\n\u001b[32m      3\u001b[39m processed = preprocessor.fit_transform(feature_df)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m processed_columns = \u001b[43mpreprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m final_df = pd.DataFrame(processed, columns=processed_columns, index=feature_df.index)\n\u001b[32m      6\u001b[39m final_df[target] = dataset[target].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/data-eda-proj/venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:633\u001b[39m, in \u001b[36mColumnTransformer.get_feature_names_out\u001b[39m\u001b[34m(self, input_features)\u001b[39m\n\u001b[32m    626\u001b[39m transformer_with_feature_names_out = []\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, trans, *_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(\n\u001b[32m    628\u001b[39m     fitted=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    629\u001b[39m     column_as_labels=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    630\u001b[39m     skip_empty_columns=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    631\u001b[39m     skip_drop=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    632\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     feature_names_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_feature_name_out_for_transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_features\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m feature_names_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    637\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/data-eda-proj/venv/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:600\u001b[39m, in \u001b[36mColumnTransformer._get_feature_name_out_for_transformer\u001b[39m\u001b[34m(self, name, trans, feature_names_in)\u001b[39m\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trans, \u001b[33m\"\u001b[39m\u001b[33mget_feature_names_out\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    596\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m    597\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTransformer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(trans).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) does \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    598\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnot provide get_feature_names_out.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    599\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrans\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_feature_names_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/data-eda-proj/venv/lib/python3.12/site-packages/sklearn/pipeline.py:1266\u001b[39m, in \u001b[36mPipeline.get_feature_names_out\u001b[39m\u001b[34m(self, input_features)\u001b[39m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter():\n\u001b[32m   1265\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transform, \u001b[33m\"\u001b[39m\u001b[33mget_feature_names_out\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1266\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1267\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mEstimator \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m does not provide get_feature_names_out. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1268\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDid you mean to call pipeline[:-1].get_feature_names_out\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1269\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m()?\u001b[39m\u001b[33m\"\u001b[39m.format(name)\n\u001b[32m   1270\u001b[39m         )\n\u001b[32m   1271\u001b[39m     feature_names_out = transform.get_feature_names_out(feature_names_out)\n\u001b[32m   1272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feature_names_out\n",
      "\u001b[31mAttributeError\u001b[39m: Estimator normalizer does not provide get_feature_names_out. Did you mean to call pipeline[:-1].get_feature_names_out()?"
     ]
    }
   ],
   "source": [
    "target = 'delivery_time_days'\n",
    "feature_df = dataset[categorical_columns + numeric_columns].copy()\n",
    "processed = preprocessor.fit_transform(feature_df)\n",
    "cat_encoder = preprocessor.named_transformers_['categorical'].named_steps['encoder']\n",
    "cat_columns_out = list(cat_encoder.get_feature_names_out(categorical_columns))\n",
    "final_columns = cat_columns_out + numeric_columns\n",
    "final_df = pd.DataFrame(processed, columns=final_columns, index=feature_df.index)\n",
    "final_df[target] = dataset[target].values\n",
    "final_df.to_csv(TARGET_FILE, index=False)\n",
    "print('Pipeline concluída e exportada para', TARGET_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ce0dc",
   "metadata": {},
   "source": [
    "### Distribuição da distância\n",
    "\n",
    "Histograma interativo com Plotly mostrando como as distâncias client-seller se comportam na base inteira."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933bc713",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(\n",
    "    dataset,\n",
    "    x='distance_km',\n",
    "    nbins=60,\n",
    "    title='Distribuição das distâncias (km) entre cliente e seller',\n",
    "    color_discrete_sequence=['#636EFA']\n",
    ")\n",
    "fig.update_layout(bargap=0.1)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a481a78",
   "metadata": {},
   "source": [
    "### Relação distância x tempo de entrega\n",
    "\n",
    "Dispersão colorida por categoria principal para inspeção visual de possíveis padrões de atraso geográfico e por produto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acd53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = px.scatter(\n",
    "    dataset,\n",
    "    x='distance_km',\n",
    "    y='delivery_time_days',\n",
    "    color='primary_category',\n",
    "    title='Tempo de entrega x distância (dados completos)',\n",
    "    opacity=0.6,\n",
    "    hover_data=['items_count', 'payment_type']\n",
    ")\n",
    "fig2.update_layout(height=600)\n",
    "fig2.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
