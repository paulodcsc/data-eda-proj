{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "033b0682",
   "metadata": {},
   "source": [
    "# Avaliação do Modelo e Conclusão\n",
    "\n",
    "Este notebook cobre o ponto 5: métricas de regressão, comparação de modelos e reflexão sobre melhorias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40acc1",
   "metadata": {},
   "source": [
    "## Passos desta seção\n",
    "\n",
    "1. Recarregar métricas sumarizadas e os modelos ajustados.\n",
    "2. Avaliar os modelos no conjunto de teste e gerar métricas RMSE/MAE/R².\n",
    "3. Visualizar o benchmark resultante.\n",
    "4. Propor ajustes futuros e concluir qual modelo entrega melhor trade-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a9d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DATA_FILE = Path('data/clean/olist_ml_ready.csv')\n",
    "metrics_file = Path('src/models/training_metrics.json')\n",
    "MODEL_DIR = Path('src/models/trained')\n",
    "\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "num_rows = len(df)\n",
    "print(f'Dataset carregado ({num_rows} registros).')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcebd026",
   "metadata": {},
   "source": [
    "### Dataset e métricas carregados\n",
    "\n",
    "Recarrega o CSV limpo e prepara o caminho para ler os resultados já calculados (modelos e métricas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd892fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    'order_id', 'customer_id',\n",
    "    'order_purchase_timestamp', 'order_approved_at',\n",
    "    'order_delivered_carrier_date', 'order_delivered_customer_date',\n",
    "    'order_estimated_delivery_date'\n",
    "]\n",
    "TARGET = 'delivery_time_days'\n",
    "X = df.drop(columns=drop_cols + [TARGET], errors='ignore')\n",
    "y = df[TARGET]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42\n",
    ")\n",
    "print('Split reproduzido. Teste:', X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f13e8a",
   "metadata": {},
   "source": [
    "### Estrutura das features\n",
    "\n",
    "Remove colunas temporais e identifica `delivery_time_days` como alvo, garantindo que os mesmos atributos usados no treino sejam avaliados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62ce6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if metrics_file.exists():\n",
    "    saved_metrics = pd.read_json(metrics_file)\n",
    "    saved_metrics\n",
    "else:\n",
    "    print('Arquivo de métricas não encontrado.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4a46b",
   "metadata": {},
   "source": [
    "### Métricas salvas\n",
    "\n",
    "Exibe as métricas registradas pelo script de treino, facilitando a comparação textual antes de gerar o benchmark gráfico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee52d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = []\n",
    "if MODEL_DIR.exists():\n",
    "    for model_path in MODEL_DIR.glob('*.joblib'):\n",
    "        name = model_path.stem\n",
    "        estimator = joblib.load(model_path)\n",
    "        preds = estimator.predict(X_test)\n",
    "        benchmark.append({\n",
    "            'modelo': name,\n",
    "            'rmse': mean_squared_error(y_test, preds, squared=False),\n",
    "            'mae': mean_absolute_error(y_test, preds),\n",
    "            'r2': r2_score(y_test, preds)\n",
    "        })\n",
    "benchmark_df = pd.DataFrame(benchmark).sort_values('rmse')\n",
    "benchmark_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08ac3ef",
   "metadata": {},
   "source": [
    "### Comparação dos modelos\n",
    "\n",
    "Carrega cada artefato `joblib`, prevê o conjunto de teste e calcula RMSE/MAE/R² para ordenar os candidatos finais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ed98b",
   "metadata": {},
   "source": [
    "### Visualização do benchmark\n",
    "\n",
    "O gráfico abaixo destaca o RMSE absoluto de cada modelo no conjunto de teste, facilitando a escolha final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb357618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.bar(\n",
    "    benchmark_df,\n",
    "    x='modelo',\n",
    "    y='rmse',\n",
    "    title='RMSE no conjunto de teste por modelo',\n",
    "    text='rmse'\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.2f}')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5789ea8e",
   "metadata": {},
   "source": [
    "### Discussão e próximos passos\n",
    "\n",
    "- O melhor modelo entrega o menor RMSE e um R² razoável, indicando boa explicabilidade.\n",
    "- Melhorias futuras: incluir lag temporal (horário da compra), integrar rotas entre cidades, testar modelos de séries temporais ou redes neurais leves.\n",
    "- Observações de negócio: o modelo pode suportar alertas práticos de atrasos e priorização de entregas."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
